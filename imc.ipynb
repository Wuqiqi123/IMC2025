{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mast3r.model import AsymmetricMASt3R\n",
    "from mast3r.fast_nn import fast_reciprocal_NNs\n",
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import copy\n",
    "from scipy.spatial.transform import Rotation\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import dataclasses\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import glob\n",
    "from mast3r.cloud_opt.sparse_ga import sparse_global_alignment\n",
    "from mast3r.cloud_opt.tsdf_optimizer import TSDFPostProcess\n",
    "from mast3r.image_pairs import make_pairs\n",
    "from mast3r.retrieval.processor import Retriever\n",
    "from mast3r.utils.misc import mkdir_for\n",
    "from cust3r.utils.image import load_images\n",
    "from dust3r.dust3r.utils.device import to_numpy\n",
    "from dust3r.dust3r.viz import add_scene_cam, CAM_COLORS, OPENGL, pts3d_to_trimesh, cat_meshes\n",
    "from dust3r.dust3r.demo import get_args_parser as dust3r_get_args_parser\n",
    "import matplotlib.pyplot as pl\n",
    "import imageio.v2 as iio\n",
    "from cust3r.utils.camera import pose_encoding_to_camera\n",
    "from cust3r.post_process import estimate_focal_knowing_depth\n",
    "from cust3r.utils.geometry import geotrf\n",
    "from cust3r.model import ARCroco3DStereo\n",
    "from cust3r.inference import inference as inference_cust3r\n",
    "import time\n",
    "from boq.boq_infer import get_trained_boq, boq_sort_topk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading model from ckpts/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\n",
      "instantiating : AsymmetricMASt3R(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100',img_size=(512, 512), head_type='catmlp+dpt', output_mode='pts3d+desc24', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), patch_embed_cls='PatchEmbedDust3R', two_confs=True, desc_conf_mode=('exp', 0, inf), landscape_only=False)\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['mask_token'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/IMC2025/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/workspace/IMC2025/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/workspace/IMC2025/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded boq model and MASt3R model successfully.\n"
     ]
    }
   ],
   "source": [
    "def _convert_scene_output_to_glb(imgs, pts3d, mask, focals, cams2world, cam_size=0.05,\n",
    "                                 cam_color=None, as_pointcloud=False,\n",
    "                                 transparent_cams=False, silent=False):\n",
    "    assert len(pts3d) == len(mask) <= len(imgs) <= len(cams2world) == len(focals)\n",
    "    pts3d = to_numpy(pts3d)\n",
    "    imgs = to_numpy(imgs)\n",
    "    focals = to_numpy(focals)\n",
    "    cams2world = to_numpy(cams2world)\n",
    "    scene = trimesh.Scene()\n",
    "    # full pointcloud\n",
    "    if as_pointcloud:\n",
    "        pts = np.concatenate([p[m.ravel()] for p, m in zip(pts3d, mask)]).reshape(-1, 3)\n",
    "        col = np.concatenate([p[m] for p, m in zip(imgs, mask)]).reshape(-1, 3)\n",
    "        valid_msk = np.isfinite(pts.sum(axis=1))\n",
    "        pct = trimesh.PointCloud(pts[valid_msk], colors=col[valid_msk])\n",
    "        scene.add_geometry(pct)\n",
    "    else:\n",
    "        meshes = []\n",
    "        for i in range(len(imgs)):\n",
    "            pts3d_i = pts3d[i].reshape(imgs[i].shape)\n",
    "            msk_i = mask[i] & np.isfinite(pts3d_i.sum(axis=-1))\n",
    "            meshes.append(pts3d_to_trimesh(imgs[i], pts3d_i, msk_i))\n",
    "        mesh = trimesh.Trimesh(**cat_meshes(meshes))\n",
    "        scene.add_geometry(mesh)\n",
    "    # add each camera\n",
    "    for i, pose_c2w in enumerate(cams2world):\n",
    "        if isinstance(cam_color, list):\n",
    "            camera_edge_color = cam_color[i]\n",
    "        else:\n",
    "            camera_edge_color = cam_color or CAM_COLORS[i % len(CAM_COLORS)]\n",
    "        add_scene_cam(scene, pose_c2w, camera_edge_color,\n",
    "                      None if transparent_cams else imgs[i], focals[i],\n",
    "                      imsize=imgs[i].shape[1::-1], screen_width=cam_size)\n",
    "    rot = np.eye(4)\n",
    "    rot[:3, :3] = Rotation.from_euler('y', np.deg2rad(180)).as_matrix()\n",
    "    scene.apply_transform(np.linalg.inv(cams2world[0] @ OPENGL @ rot))\n",
    "    return scene\n",
    "\n",
    "def get_3D_model_from_scene(silent, scene, min_conf_thr=2, as_pointcloud=False, mask_sky=False,\n",
    "                            clean_depth=False, transparent_cams=False, cam_size=0.05, TSDF_thresh=0):\n",
    "    \"\"\"\n",
    "    extract 3D_model (glb file) from a reconstructed scene\n",
    "    \"\"\"\n",
    "    # get optimized values from scene\n",
    "    scene = scene\n",
    "    rgbimg = scene.imgs\n",
    "    focals = scene.get_focals().cpu()\n",
    "    cams2world = scene.get_im_poses().cpu()\n",
    "    # 3D pointcloud from depthmap, poses and intrinsics\n",
    "    if TSDF_thresh > 0:\n",
    "        tsdf = TSDFPostProcess(scene, TSDF_thresh=TSDF_thresh)\n",
    "        pts3d, _, confs = to_numpy(tsdf.get_dense_pts3d(clean_depth=clean_depth))\n",
    "    else:\n",
    "        pts3d, _, confs = to_numpy(scene.get_dense_pts3d(clean_depth=clean_depth))\n",
    "    msk = to_numpy([c > min_conf_thr for c in confs])\n",
    "    return _convert_scene_output_to_glb(rgbimg, pts3d, msk, focals, cams2world, as_pointcloud=as_pointcloud,\n",
    "                                        transparent_cams=transparent_cams, cam_size=cam_size, silent=silent)\n",
    "    \n",
    "\n",
    "def get_reconstructed_scene(model, device, filelist,\n",
    "                            cache_path,\n",
    "                            retrieval_model = None,\n",
    "                            silent = False,\n",
    "                            optim_level = \"refine+depth\",\n",
    "                            lr1 = 0.07, niter1 = 200, lr2 = 0.01, niter2 = 200,\n",
    "                            min_conf_thr = 1.5,\n",
    "                            matching_conf_thr = 0.0,\n",
    "                            as_pointcloud = True, mask_sky = False, clean_depth =True, transparent_cams = False, cam_size = 0.2,\n",
    "                            scenegraph_type = \"complete\", winsize=1, win_cyclic=False, refid=0,\n",
    "                            TSDF_thresh=0.0, shared_intrinsics= False,\n",
    "                            trimesh_scenes=False,\n",
    "                            **kw):\n",
    "    \"\"\"\n",
    "    from a list of images, run mast3r inference, sparse global aligner.\n",
    "    then run get_3D_model_from_scene\n",
    "    \"\"\"\n",
    "    imgs, imgs_id_dict = load_images(filelist, size=512, verbose=not silent)\n",
    "    if len(imgs) == 1:\n",
    "        imgs = [imgs[0], copy.deepcopy(imgs[0])]\n",
    "        imgs[1]['idx'] = 1\n",
    "        filelist = [filelist[0], filelist[0] + '_2']\n",
    "    scene_graph_params = [scenegraph_type]\n",
    "    if scenegraph_type in [\"swin\", \"logwin\"]:\n",
    "        scene_graph_params.append(str(winsize))\n",
    "    elif scenegraph_type == \"oneref\":\n",
    "        scene_graph_params.append(str(refid))\n",
    "    elif scenegraph_type == \"retrieval\":\n",
    "        scene_graph_params.append(str(winsize))  # Na\n",
    "        scene_graph_params.append(str(refid))  # k\n",
    "    if scenegraph_type in [\"swin\", \"logwin\"] and not win_cyclic:\n",
    "        scene_graph_params.append('noncyclic')\n",
    "    scene_graph = '-'.join(scene_graph_params)\n",
    "    sim_matrix = None\n",
    "    if 'retrieval' in scenegraph_type:\n",
    "        assert retrieval_model is not None\n",
    "        retriever = Retriever(retrieval_model, backbone=model, device=device)\n",
    "        with torch.no_grad():\n",
    "            sim_matrix = retriever(filelist)\n",
    "        # Cleanup\n",
    "        del retriever\n",
    "        torch.cuda.empty_cache()\n",
    "    boq_topks = None\n",
    "    if 'boq' in scenegraph_type:\n",
    "        with open(os.path.join(cache_path, \"boq_topk.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            boq_topks = json.load(f)\n",
    "    pairs = make_pairs(imgs, scene_graph=scene_graph, prefilter=None, \n",
    "                       symmetrize=False, sim_mat=sim_matrix, boq_topk_dict=boq_topks, \n",
    "                       imgs_id_dict=imgs_id_dict)\n",
    "    if optim_level == 'coarse':\n",
    "        niter2 = 0\n",
    "    # Sparse GA (forward mast3r -> matching -> 3D optim -> 2D refinement -> triangulation)\n",
    "    scenes, outlier_imgs = sparse_global_alignment(filelist, imgs, imgs_id_dict, pairs, cache_path,\n",
    "                                    model, lr1=lr1, niter1=niter1, lr2=lr2, niter2=niter2, device=device,\n",
    "                                    opt_depth='depth' in optim_level, shared_intrinsics=shared_intrinsics,\n",
    "                                    matching_conf_thr=matching_conf_thr, **kw)\n",
    "    if trimesh_scenes:\n",
    "        trimesh_scenes = []\n",
    "        for i, scene in enumerate(scenes):\n",
    "            trimesh_scene = get_3D_model_from_scene(silent, scene, min_conf_thr, as_pointcloud, mask_sky,\n",
    "                                        clean_depth, transparent_cams, cam_size, TSDF_thresh)\n",
    "            trimesh_scenes.append(trimesh_scene)\n",
    "        return trimesh_scenes, outlier_imgs\n",
    "    else:\n",
    "        return scenes, outlier_imgs\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda:0'\n",
    "model = AsymmetricMASt3R.from_pretrained(\"ckpts/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\").to(device)\n",
    "\n",
    "boq_model = get_trained_boq(backbone_name=\"dinov2\", output_dim=12288, ckpt='ckpts/dinov2_12288.pth').to(device)\n",
    "boq_model.eval()\n",
    "print(\"Loaded boq model and MASt3R model successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"stairs\" -> num_images=51\n",
      "Images dir: data/image-matching-challenge-2025/test/ETs\n",
      "\n",
      "Processing dataset \"ETs\": 22 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 49.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find topk time: 0.00017690658569335938 s\n",
      ">> Loading a list of 22 images\n",
      " - adding data/image-matching-challenge-2025/test/ETs/another_et_another_et001.png with resolution 360x640 --> 288x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/another_et_another_et002.png with resolution 360x640 --> 288x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/another_et_another_et003.png with resolution 360x640 --> 288x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/another_et_another_et004.png with resolution 360x640 --> 288x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/another_et_another_et005.png with resolution 360x640 --> 288x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/another_et_another_et006.png with resolution 360x640 --> 288x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/another_et_another_et007.png with resolution 360x640 --> 288x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/another_et_another_et008.png with resolution 360x640 --> 288x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/another_et_another_et009.png with resolution 360x640 --> 288x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/another_et_another_et010.png with resolution 360x640 --> 288x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/et_et000.png with resolution 480x640 --> 384x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/et_et001.png with resolution 480x640 --> 384x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/et_et002.png with resolution 480x640 --> 384x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/et_et003.png with resolution 480x640 --> 384x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/et_et004.png with resolution 480x640 --> 384x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/et_et005.png with resolution 480x640 --> 384x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/et_et006.png with resolution 480x640 --> 384x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/et_et007.png with resolution 480x640 --> 384x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/et_et008.png with resolution 480x640 --> 384x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/outliers_out_et001.png with resolution 262x450 --> 288x512\n",
      " - adding data/image-matching-challenge-2025/test/ETs/outliers_out_et002.png with resolution 300x300 --> 512x384\n",
      " - adding data/image-matching-challenge-2025/test/ETs/outliers_out_et003.png with resolution 344x500 --> 352x512\n",
      " (Found 22 images)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:00<00:00, 20041.87it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters = [3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 1 2 5]\n",
      "cluster 3:\n",
      "-- another_et_another_et001\n",
      "-- another_et_another_et002\n",
      "-- another_et_another_et003\n",
      "-- another_et_another_et004\n",
      "-- another_et_another_et005\n",
      "-- another_et_another_et006\n",
      "-- another_et_another_et007\n",
      "-- another_et_another_et008\n",
      "-- another_et_another_et009\n",
      "-- another_et_another_et010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 16415.35it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 38.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init focals = [428.2006  465.46454 480.01993 477.07062 463.4202  448.05655 451.65335\n",
      " 458.37122 445.2563  426.69232]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 32.45it/s, lr=0.0000, loss=0.177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> final loss = 0.17658554017543793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:08<00:00, 24.13it/s, lr=0.0000, loss=0.824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> final loss = 0.8242846131324768\n",
      "Final focals = [468.37222 465.94443 462.9056  462.36932 464.8395  466.79614 466.09976\n",
      " 464.78543 460.99387 453.83362]\n",
      "cluster 4:\n",
      "-- et_et000\n",
      "-- et_et001\n",
      "-- et_et002\n",
      "-- et_et003\n",
      "-- et_et004\n",
      "-- et_et005\n",
      "-- et_et006\n",
      "-- et_et007\n",
      "-- et_et008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:00<00:00, 17331.83it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 36.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init focals = [498.44272 500.10925 497.4111  507.90295 481.6423  475.06934 465.79468\n",
      " 469.42758 466.40543]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 40.06it/s, lr=0.0000, loss=0.113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> final loss = 0.11304111033678055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.93it/s, lr=0.0000, loss=0.693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> final loss = 0.6930036544799805\n",
      "Final focals = [522.40857 520.1605  519.3459  523.5777  521.3308  516.6578  516.94684\n",
      " 515.25867 513.94855]\n",
      "cluster 1:\n",
      "-- outliers_out_et001\n",
      "cluster 2:\n",
      "-- outliers_out_et002\n",
      "cluster 5:\n",
      "-- outliers_out_et003\n",
      "Dataset \"ETs\" -> Registered 19 / 22 images with 2 clusters\n",
      "Images dir \"data/image-matching-challenge-2025/test/amy_gardens\" does not exist. Skipping \"amy_gardens\"\n",
      "Images dir \"data/image-matching-challenge-2025/test/fbk_vineyard\" does not exist. Skipping \"fbk_vineyard\"\n",
      "Images dir \"data/image-matching-challenge-2025/test/imc2023_haiper\" does not exist. Skipping \"imc2023_haiper\"\n",
      "Images dir \"data/image-matching-challenge-2025/test/imc2023_heritage\" does not exist. Skipping \"imc2023_heritage\"\n",
      "Images dir \"data/image-matching-challenge-2025/test/imc2023_theather_imc2024_church\" does not exist. Skipping \"imc2023_theather_imc2024_church\"\n",
      "Images dir \"data/image-matching-challenge-2025/test/imc2024_dioscuri_baalshamin\" does not exist. Skipping \"imc2024_dioscuri_baalshamin\"\n",
      "Images dir \"data/image-matching-challenge-2025/test/imc2024_lizard_pond\" does not exist. Skipping \"imc2024_lizard_pond\"\n",
      "Images dir \"data/image-matching-challenge-2025/test/pt_brandenburg_british_buckingham\" does not exist. Skipping \"pt_brandenburg_british_buckingham\"\n",
      "Images dir \"data/image-matching-challenge-2025/test/pt_piazzasanmarco_grandplace\" does not exist. Skipping \"pt_piazzasanmarco_grandplace\"\n",
      "Images dir \"data/image-matching-challenge-2025/test/pt_sacrecoeur_trevi_tajmahal\" does not exist. Skipping \"pt_sacrecoeur_trevi_tajmahal\"\n",
      "Images dir \"data/image-matching-challenge-2025/test/pt_stpeters_stpauls\" does not exist. Skipping \"pt_stpeters_stpauls\"\n",
      "Images dir: data/image-matching-challenge-2025/test/stairs\n",
      "\n",
      "Processing dataset \"stairs\": 51 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find topk time: 0.00016808509826660156 s\n",
      ">> Loading a list of 51 images\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453576271.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453601885.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453606287.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453612890.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453616892.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453620694.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453626698.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453643106.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453651110.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453659313.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453663515.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453667117.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453668718.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453675921.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453678922.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453683725.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453689727.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453693529.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453697531.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453704934.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453901046.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453912451.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453930259.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453947066.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453955270.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453963274.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453985484.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_1_1710453990286.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453720741.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453725143.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453728949.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453733751.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453736752.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453739354.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453740954.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453745156.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453753160.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453756762.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453759963.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453765165.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453774370.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453779372.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453783374.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453786375.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453790978.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453793579.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453798181.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453801783.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453805788.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453862225.png with resolution 1280x1024 --> 512x400\n",
      " - adding data/image-matching-challenge-2025/test/stairs/stairs_split_2_1710453871430.png with resolution 1280x1024 --> 512x400\n",
      " (Found 51 images)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:00<00:00, 21156.88it/s]\n",
      "100%|██████████| 51/51 [00:19<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters = [2 2 6 6 5 5 7 4 1 6 6 1 1 8 8 4 1 1 2 2 2 4 8 2 1 7 6 2 1 2 3 3 1 1 1 2 2\n",
      " 1 1 2 3 1 1 1 2 2 2 1 1 2 1]\n",
      "cluster 2:\n",
      "-- stairs_split_1_1710453576271\n",
      "-- stairs_split_1_1710453601885\n",
      "-- stairs_split_1_1710453697531\n",
      "-- stairs_split_1_1710453704934\n",
      "-- stairs_split_1_1710453901046\n",
      "-- stairs_split_1_1710453947066\n",
      "-- stairs_split_1_1710453990286\n",
      "-- stairs_split_2_1710453725143\n",
      "-- stairs_split_2_1710453745156\n",
      "-- stairs_split_2_1710453753160\n",
      "-- stairs_split_2_1710453765165\n",
      "-- stairs_split_2_1710453790978\n",
      "-- stairs_split_2_1710453793579\n",
      "-- stairs_split_2_1710453798181\n",
      "-- stairs_split_2_1710453862225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:00<00:00, 19500.62it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 23.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init focals = [327.44305 333.62738 303.10373 316.44193 329.03528 310.73373 319.1424\n",
      " 322.82477 318.91376 330.83618 324.36996 310.00577 329.86963 325.5856\n",
      " 333.39645]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.56it/s, lr=0.0000, loss=0.354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> final loss = 0.35404425859451294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:11<00:00, 16.94it/s, lr=0.0000, loss=2.506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> final loss = 2.5055806636810303\n",
      "Final focals = [321.0583  333.2399  319.69608 334.6209  335.2054  328.35208 332.79703\n",
      " 338.4959  325.9628  229.46794 332.8075  319.63095 319.39435 328.09744\n",
      " 610.03516]\n",
      "cluster 6:\n",
      "-- stairs_split_1_1710453606287\n",
      "-- stairs_split_1_1710453612890\n",
      "-- stairs_split_1_1710453659313\n",
      "-- stairs_split_1_1710453663515\n",
      "-- stairs_split_1_1710453985484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12365.28it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 67.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init focals = [325.5159  319.04257 324.7632  338.32568 323.60678]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 53.79it/s, lr=0.0000, loss=0.268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> final loss = 0.26760053634643555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 33.30it/s, lr=0.0000, loss=0.936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> final loss = 0.9360532760620117\n",
      "Final focals = [345.88885 338.91278 341.82263 356.78943 346.14313]\n",
      "cluster 5:\n",
      "-- stairs_split_1_1710453616892\n",
      "-- stairs_split_1_1710453620694\n",
      "cluster 7:\n",
      "-- stairs_split_1_1710453626698\n",
      "-- stairs_split_1_1710453963274\n",
      "cluster 4:\n",
      "-- stairs_split_1_1710453643106\n",
      "-- stairs_split_1_1710453683725\n",
      "-- stairs_split_1_1710453912451\n",
      "cluster 1:\n",
      "-- stairs_split_1_1710453651110\n",
      "-- stairs_split_1_1710453667117\n",
      "-- stairs_split_1_1710453668718\n",
      "-- stairs_split_1_1710453689727\n",
      "-- stairs_split_1_1710453693529\n",
      "-- stairs_split_1_1710453955270\n",
      "-- stairs_split_2_1710453720741\n",
      "-- stairs_split_2_1710453736752\n",
      "-- stairs_split_2_1710453739354\n",
      "-- stairs_split_2_1710453740954\n",
      "-- stairs_split_2_1710453756762\n",
      "-- stairs_split_2_1710453759963\n",
      "-- stairs_split_2_1710453779372\n",
      "-- stairs_split_2_1710453783374\n",
      "-- stairs_split_2_1710453786375\n",
      "-- stairs_split_2_1710453801783\n",
      "-- stairs_split_2_1710453805788\n",
      "-- stairs_split_2_1710453871430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:00<00:00, 11239.27it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 18.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init focals = [299.93994 334.3772  299.1542  321.7056  298.5823  330.25333 305.85776\n",
      " 301.8666  297.1641  309.96954 293.34232 287.30527 333.4496  293.1615\n",
      " 304.14297 310.9521  293.14822 309.2224 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:08<00:00, 24.99it/s, lr=0.0000, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> final loss = 0.21108736097812653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:13<00:00, 14.49it/s, lr=0.0000, loss=3.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> final loss = 3.3279833793640137\n",
      "Final focals = [317.39804 333.0115  317.8615  313.25854 314.6121  465.81888 323.7293\n",
      " 312.07205 312.3381  318.2786  303.41306 309.4354  331.745   305.86972\n",
      " 314.29315 322.0563  307.77563 307.8875 ]\n",
      "cluster 8:\n",
      "-- stairs_split_1_1710453675921\n",
      "-- stairs_split_1_1710453678922\n",
      "-- stairs_split_1_1710453930259\n",
      "cluster 3:\n",
      "-- stairs_split_2_1710453728949\n",
      "-- stairs_split_2_1710453733751\n",
      "-- stairs_split_2_1710453774370\n",
      "Dataset \"stairs\" -> Registered 38 / 51 images with 3 clusters\n"
     ]
    }
   ],
   "source": [
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = False\n",
    "data_dir = 'data/image-matching-challenge-2025'\n",
    "workdir = 'result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "workdir = Path(workdir)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t# 'amy_gardens',\n",
    "    \t'ETs',\n",
    "    \t# 'fbk_vineyard',\n",
    "    \t'stairs',\n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t# 'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "for dataset, predictions in samples.items():\n",
    "    if datasets_to_process and dataset not in datasets_to_process:\n",
    "        print(f'Skipping \"{dataset}\"')\n",
    "        continue\n",
    "    \n",
    "    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "    if not os.path.exists(images_dir):\n",
    "        print(f'Images dir \"{images_dir}\" does not exist. Skipping \"{dataset}\"')\n",
    "        continue\n",
    "    \n",
    "    images_dir = Path(images_dir)\n",
    "\n",
    "    print(f'Images dir: {images_dir}')\n",
    "\n",
    "    image_names = [p.filename for p in predictions]\n",
    "    if max_images is not None:\n",
    "        image_names = image_names[:max_images]\n",
    "\n",
    "    image_list = [os.path.join(images_dir, name) for name in image_names]\n",
    "\n",
    "    print(f'\\nProcessing dataset \"{dataset}\": {len(image_names)} images')\n",
    "\n",
    "    dataset_dir = os.path.join(workdir, dataset)\n",
    "    boq_topks = boq_sort_topk(image_list, boq_model, device, vis=False, topk=32)\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    with open(os.path.join(dataset_dir, \"boq_topk.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(boq_topks, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    scenes, outlier_imgs = get_reconstructed_scene(model, device, image_list, dataset_dir, scenegraph_type = \"boq\")\n",
    "\n",
    "    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "\n",
    "    registered = 0\n",
    "    for map_index, cur_map in enumerate(scenes):\n",
    "        cams2world = cur_map.get_im_poses().cpu()\n",
    "        for image_index, image_path in enumerate(cur_map.img_paths):\n",
    "            image_name = os.path.basename(image_path)\n",
    "            prediction_index = filename_to_index[image_name]\n",
    "            predictions[prediction_index].cluster_index = map_index\n",
    "            world2cam = np.linalg.inv(cams2world[image_index])\n",
    "            predictions[prediction_index].rotation = world2cam[:3, :3]\n",
    "            predictions[prediction_index].translation = world2cam[:3, 3]\n",
    "            registered += 1\n",
    "    mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(image_names)} images with {len(scenes)} clusters'\n",
    "    print(mapping_result_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2227.66s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset,scene,image,rotation_matrix,translation_vector\n",
      "ETs,cluster0,another_et_another_et001.png,0.829320967;-0.296454519;0.473535150;0.407318354;0.901143014;-0.149280593;-0.382423401;0.316625416;0.868176877,-0.721401930;0.208028257;2.042875290\n",
      "ETs,cluster0,another_et_another_et002.png,0.836093187;-0.299573362;0.459783375;0.415738553;0.892667055;-0.174442753;-0.358136207;0.336775690;0.870745301,-0.723151922;0.274954557;1.812339783\n",
      "ETs,cluster0,another_et_another_et003.png,0.800178468;-0.317904115;0.508577645;0.473317087;0.855510652;-0.209934086;-0.368354708;0.408703238;0.835030735,-0.688192189;0.369250059;1.581452131\n",
      "ETs,cluster0,another_et_another_et004.png,0.818103492;-0.295061767;0.493105322;0.367004961;0.928617418;-0.053438287;-0.442248493;0.224728599;0.868099332,-0.681945086;0.449573457;1.548249960\n",
      "ETs,cluster0,another_et_another_et005.png,0.798109055;-0.260332584;0.543678641;0.378932685;0.917749643;-0.116935670;-0.468644619;0.299516171;0.831437707,-0.647862315;0.257036418;1.819392085\n",
      "ETs,cluster0,another_et_another_et006.png,0.972194552;-0.219590291;0.084759645;0.234879330;0.922161520;-0.306906193;-0.010857508;0.318221837;0.948077083,-0.880192816;0.333912164;1.630306244\n",
      "ETs,cluster0,another_et_another_et007.png,0.961644232;-0.193123505;-0.196020663;0.128986195;0.945870936;-0.298352748;0.242536992;0.261582792;0.934418380,-0.933426976;0.484200627;1.308118820\n",
      "ETs,cluster0,another_et_another_et008.png,0.863196969;-0.155941948;-0.481112391;0.022171805;0.962243855;-0.271821737;0.504733980;0.223619103;0.833605170,-0.925749183;0.560834229;1.088276505\n",
      "ETs,cluster0,another_et_another_et009.png,0.698413730;-0.091975294;-0.710273981;-0.110514499;0.965853274;-0.233510599;0.706950426;0.241321728;0.664548457,-0.785972416;0.655711710;0.856676638\n"
     ]
    }
   ],
   "source": [
    "# Must Create a submission file.\n",
    "\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = 'result/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "\n",
    "!head {submission_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imc2023_haiper: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2023_heritage: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2023_theather_imc2024_church: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2024_dioscuri_baalshamin: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2024_lizard_pond: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_brandenburg_british_buckingham: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_piazzasanmarco_grandplace: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_sacrecoeur_trevi_tajmahal: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_stpeters_stpauls: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "amy_gardens: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "fbk_vineyard: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "ETs: score=26.67% (mAA=15.38%, clusterness=100.00%)\n",
      "stairs: score=2.18% (mAA=1.11%, clusterness=57.58%)\n",
      "Average over all datasets: score=2.22% (mAA=1.27%, clusterness=12.12%)\n"
     ]
    }
   ],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "is_train = True\n",
    "if is_train:\n",
    "    import metric\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='data/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='data/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
