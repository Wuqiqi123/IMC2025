{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2370d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"vggt/\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "from vggt.vggt.models.vggt import VGGT\n",
    "from vggt.vggt.utils.rotation import mat_to_quat\n",
    "from vggt.vggt.utils.load_fn import load_and_preprocess_images\n",
    "from vggt.vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "from vggt.vggt.utils.geometry import closed_form_inverse_se3\n",
    "import argparse\n",
    "\n",
    "import pycolmap\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from vggt.vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "from vggt.vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "from vggt.evaluation.tensor_to_pycolmap import batch_matrix_to_pycolmap, pycolmap_to_batch_matrix\n",
    "\n",
    "from lightglue import ALIKED, SuperPoint, SIFT\n",
    "\n",
    "# Suppress DINO v2 logs\n",
    "logging.getLogger(\"dinov2\").setLevel(logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\", message=\"xFormers is available\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"dinov2\")\n",
    "\n",
    "# Set computation precision\n",
    "torch.set_float32_matmul_precision('highest')\n",
    "torch.backends.cudnn.allow_tf32 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d938829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_RESNET_MEAN = [0.485, 0.456, 0.406]\n",
    "_RESNET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "def generate_rank_by_dino(\n",
    "    images, query_frame_num, image_size=518, model_name=\"dinov2_vitb14_reg\", spatial_similarity=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a ranking of frames using DINO ViT features.\n",
    "\n",
    "    Args:\n",
    "        images: Tensor of shape (S, 3, H, W) with values in range [0, 1]\n",
    "        query_frame_num: Number of frames to select\n",
    "        image_size: Size to resize images to before processing\n",
    "        model_name: Name of the DINO model to use\n",
    "        device: Device to run the model on\n",
    "        spatial_similarity: Whether to use spatial token similarity or CLS token similarity\n",
    "\n",
    "    Returns:\n",
    "        List of frame indices ranked by their representativeness\n",
    "    \"\"\"\n",
    "    device = \"cuda:0\"\n",
    "    dino_v2_model = torch.hub.load('facebookresearch/dinov2', model_name)\n",
    "    dino_v2_model.eval()\n",
    "    dino_v2_model = dino_v2_model.to(device)\n",
    "\n",
    "    resnet_mean = torch.tensor(_RESNET_MEAN, device=device).view(1, 3, 1, 1)\n",
    "    resnet_std = torch.tensor(_RESNET_STD, device=device).view(1, 3, 1, 1)\n",
    "    images = images.to(device)\n",
    "    images_resnet_norm = (images - resnet_mean) / resnet_std\n",
    "\n",
    "    with torch.no_grad():\n",
    "        frame_feat = dino_v2_model(images_resnet_norm, is_training=True)\n",
    "\n",
    "    if spatial_similarity:\n",
    "        frame_feat = frame_feat[\"x_norm_patchtokens\"]\n",
    "        frame_feat_norm = F.normalize(frame_feat, p=2, dim=1)\n",
    "\n",
    "        # Compute the similarity matrix\n",
    "        frame_feat_norm = frame_feat_norm.permute(1, 0, 2)\n",
    "        similarity_matrix = torch.bmm(\n",
    "            frame_feat_norm, frame_feat_norm.transpose(-1, -2)\n",
    "        )\n",
    "        similarity_matrix = similarity_matrix.mean(dim=0)\n",
    "    else:\n",
    "        frame_feat = frame_feat[\"x_norm_clstoken\"]\n",
    "        frame_feat_norm = F.normalize(frame_feat, p=2, dim=1)\n",
    "        similarity_matrix = torch.mm(\n",
    "            frame_feat_norm, frame_feat_norm.transpose(-1, -2)\n",
    "        )\n",
    "\n",
    "    distance_matrix = 100 - similarity_matrix.clone()\n",
    "\n",
    "    # Ignore self-pairing\n",
    "    similarity_matrix.fill_diagonal_(-100)\n",
    "    similarity_sum = similarity_matrix.sum(dim=1)\n",
    "\n",
    "    # Find the most common frame\n",
    "    most_common_frame_index = torch.argmax(similarity_sum).item()\n",
    "\n",
    "    most_common_frame_index = most_common_frame_index\n",
    "\n",
    "    # Conduct FPS sampling starting from the most common frame\n",
    "    fps_idx = farthest_point_sampling(\n",
    "        distance_matrix, query_frame_num, most_common_frame_index\n",
    "    )\n",
    "\n",
    "    return fps_idx\n",
    "\n",
    "\n",
    "def farthest_point_sampling(\n",
    "    distance_matrix, num_samples, most_common_frame_index=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Farthest point sampling algorithm to select diverse frames.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix: Matrix of distances between frames\n",
    "        num_samples: Number of frames to select\n",
    "        most_common_frame_index: Index of the first frame to select\n",
    "\n",
    "    Returns:\n",
    "        List of selected frame indices\n",
    "    \"\"\"\n",
    "    distance_matrix = distance_matrix.clamp(min=0)\n",
    "    N = distance_matrix.size(0)\n",
    "\n",
    "    # Initialize with the most common frame\n",
    "    selected_indices = [most_common_frame_index]\n",
    "    check_distances = distance_matrix[selected_indices]\n",
    "\n",
    "    while len(selected_indices) < num_samples:\n",
    "        # Find the farthest point from the current set of selected points\n",
    "        farthest_point = torch.argmax(check_distances)\n",
    "        selected_indices.append(farthest_point.item())\n",
    "\n",
    "        check_distances = distance_matrix[farthest_point]\n",
    "        # Mark already selected points to avoid selecting them again\n",
    "        check_distances[selected_indices] = 0\n",
    "\n",
    "        # Break if all points have been selected\n",
    "        if len(selected_indices) == N:\n",
    "            break\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "def calculate_index_mappings(query_index, S, device=None):\n",
    "    \"\"\"\n",
    "    Construct an order that switches [query_index] and [0]\n",
    "    so that the content of query_index would be placed at [0].\n",
    "\n",
    "    Args:\n",
    "        query_index: Index to swap with 0\n",
    "        S: Total number of elements\n",
    "        device: Device to place the tensor on\n",
    "\n",
    "    Returns:\n",
    "        Tensor of indices with the swapped order\n",
    "    \"\"\"\n",
    "    new_order = torch.arange(S)\n",
    "    new_order[0] = query_index\n",
    "    new_order[query_index] = 0\n",
    "    if device is not None:\n",
    "        new_order = new_order.to(device)\n",
    "    return new_order\n",
    "\n",
    "\n",
    "def switch_tensor_order(tensors, order, dim=1):\n",
    "    \"\"\"\n",
    "    Reorder tensors along a specific dimension according to the given order.\n",
    "\n",
    "    Args:\n",
    "        tensors: List of tensors to reorder\n",
    "        order: Tensor of indices specifying the new order\n",
    "        dim: Dimension along which to reorder\n",
    "\n",
    "    Returns:\n",
    "        List of reordered tensors\n",
    "    \"\"\"\n",
    "    return [\n",
    "        torch.index_select(tensor, dim, order) if tensor is not None else None\n",
    "        for tensor in tensors\n",
    "    ]\n",
    "\n",
    "\n",
    "def predict_track(model, images, query_points, dtype=torch.bfloat16, use_tf32_for_track=True, iters=4):\n",
    "    \"\"\"\n",
    "    Predict tracks for query points across frames.\n",
    "\n",
    "    Args:\n",
    "        model: VGGT model\n",
    "        images: Tensor of images of shape (S, 3, H, W)\n",
    "        query_points: Query points to track\n",
    "        dtype: Data type for computation\n",
    "        use_tf32_for_track: Whether to use TF32 precision for tracking\n",
    "        iters: Number of iterations for tracking\n",
    "\n",
    "    Returns:\n",
    "        Predicted tracks, visibility scores, and confidence scores\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(dtype=dtype):\n",
    "            images = images[None]  # add batch dimension\n",
    "            aggregated_tokens_list, ps_idx = model.aggregator(images)\n",
    "\n",
    "            if not use_tf32_for_track:\n",
    "                track_list, vis_score, conf_score = model.track_head(\n",
    "                    aggregated_tokens_list, images, ps_idx, query_points=query_points, iters=iters\n",
    "                )\n",
    "\n",
    "        if use_tf32_for_track:\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                track_list, vis_score, conf_score = model.track_head(\n",
    "                    aggregated_tokens_list, images, ps_idx, query_points=query_points, iters=iters\n",
    "                )\n",
    "\n",
    "    pred_track = track_list[-1]\n",
    "\n",
    "    return pred_track.squeeze(0), vis_score.squeeze(0), conf_score.squeeze(0)\n",
    "\n",
    "\n",
    "def initialize_feature_extractors(max_query_num, det_thres, extractor_method=\"aliked\", device=\"cuda:0\"):\n",
    "    \"\"\"\n",
    "    Initialize feature extractors that can be reused based on a method string.\n",
    "\n",
    "    Args:\n",
    "        max_query_num: Maximum number of keypoints to extract\n",
    "        det_thres: Detection threshold for keypoint extraction\n",
    "        extractor_method: String specifying which extractors to use (e.g., \"aliked\", \"sp+sift\", \"aliked+sp+sift\")\n",
    "        device: Device to run extraction on\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of initialized extractors\n",
    "    \"\"\"\n",
    "    extractors = {}\n",
    "    methods = extractor_method.lower().split('+')\n",
    "\n",
    "    for method in methods:\n",
    "        method = method.strip()\n",
    "        if method == \"aliked\":\n",
    "            aliked_extractor = ALIKED(max_num_keypoints=max_query_num, detection_threshold=det_thres)\n",
    "            extractors['aliked'] = aliked_extractor.to(device).eval()\n",
    "        elif method == \"sp\":\n",
    "            sp_extractor = SuperPoint(max_num_keypoints=max_query_num, detection_threshold=det_thres)\n",
    "            extractors['sp'] = sp_extractor.to(device).eval()\n",
    "        elif method == \"sift\":\n",
    "            sift_extractor = SIFT(max_num_keypoints=max_query_num)\n",
    "            extractors['sift'] = sift_extractor.to(device).eval()\n",
    "        else:\n",
    "            print(f\"Warning: Unknown feature extractor '{method}', ignoring.\")\n",
    "\n",
    "    if not extractors:\n",
    "        print(f\"Warning: No valid extractors found in '{extractor_method}'. Using ALIKED by default.\")\n",
    "        aliked_extractor = ALIKED(max_num_keypoints=max_query_num, detection_threshold=det_thres)\n",
    "        extractors['aliked'] = aliked_extractor.to(device).eval()\n",
    "\n",
    "    return extractors\n",
    "\n",
    "\n",
    "def extract_keypoints(query_image, extractors, max_query_num):\n",
    "    \"\"\"\n",
    "    Extract keypoints using pre-initialized feature extractors.\n",
    "\n",
    "    Args:\n",
    "        query_image: Input image tensor (3xHxW, range [0, 1])\n",
    "        extractors: Dictionary of initialized extractors\n",
    "\n",
    "    Returns:\n",
    "        Tensor of keypoint coordinates (1xNx2)\n",
    "    \"\"\"\n",
    "    query_points_round = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for extractor_name, extractor in extractors.items():\n",
    "            query_points_data = extractor.extract(query_image)\n",
    "            extractor_points = query_points_data[\"keypoints\"].round()\n",
    "\n",
    "            if query_points_round is not None:\n",
    "                query_points_round = torch.cat([query_points_round, extractor_points], dim=1)\n",
    "            else:\n",
    "                query_points_round = extractor_points\n",
    "\n",
    "    if query_points_round.shape[1] > max_query_num:\n",
    "        random_point_indices = torch.randperm(query_points_round.shape[1])[\n",
    "            :max_query_num\n",
    "        ]\n",
    "        query_points_round = query_points_round[:, random_point_indices, :]\n",
    "\n",
    "    return query_points_round\n",
    "\n",
    "\n",
    "def run_vggt_with_ba(\n",
    "    model,\n",
    "    images,\n",
    "    image_names=None,\n",
    "    dtype=torch.bfloat16,\n",
    "    max_query_num=2048,\n",
    "    det_thres=0.005,\n",
    "    query_frame_num=3,\n",
    "    extractor_method=\"aliked+sp+sift\",\n",
    "    max_reproj_error=12,\n",
    "    shared_camera=True,\n",
    "    camera_type=\"SIMPLE_PINHOLE\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run VGGT with bundle adjustment for pose estimation.\n",
    "\n",
    "    Args:\n",
    "        model: VGGT model for feature extraction and tracking\n",
    "        images: Tensor of images of shape (S, 3, H, W)\n",
    "        image_names: Optional list of image names\n",
    "        dtype: Data type for computation (default: torch.bfloat16)\n",
    "        max_query_num: Maximum number of query points to track (default: 2048)\n",
    "        det_thres: Detection threshold for keypoint extraction (default: 0.005)\n",
    "        query_frame_num: Number of frames to select for feature extraction (default: 3)\n",
    "        extractor_method: Feature extraction method (default: \"aliked+sp+sift\")\n",
    "        max_reproj_error: Maximum reprojection error for bundle adjustment (default: 12)\n",
    "        shared_camera: Whether to use shared camera parameters (default: True)\n",
    "        camera_type: Camera model type (default: \"SIMPLE_PINHOLE\")\n",
    "\n",
    "    Returns:\n",
    "        Predicted extrinsic camera parameters\n",
    "\n",
    "    TODO:\n",
    "        - [ ] Use VGGT's vit instead of dinov2 for rank generation\n",
    "    \"\"\"\n",
    "\n",
    "    assert \"RADIAL\" not in camera_type, \"RADIAL camera is not supported yet\"\n",
    "\n",
    "    device = images.device\n",
    "    frame_num = images.shape[0]\n",
    "\n",
    "    # Select representative frames for feature extraction\n",
    "    query_frame_indexes = generate_rank_by_dino(\n",
    "        images, query_frame_num, image_size=518,\n",
    "        model_name=\"dinov2_vitb14_reg\",\n",
    "        spatial_similarity=False\n",
    "    )\n",
    "\n",
    "    # Add the first image to the front if not already present\n",
    "    if 0 in query_frame_indexes:\n",
    "        query_frame_indexes.remove(0)\n",
    "    query_frame_indexes = [0, *query_frame_indexes]\n",
    "\n",
    "    # Get initial pose and depth predictions\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(dtype=dtype):\n",
    "            predictions = model(images) # TODO: point map head is redundant here, remove it\n",
    "\n",
    "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        extrinsic, intrinsic = pose_encoding_to_extri_intri(predictions[\"pose_enc\"], images.shape[-2:])\n",
    "        pred_extrinsic = extrinsic[0]\n",
    "        pred_intrinsic = intrinsic[0]\n",
    "\n",
    "        # Get 3D points from depth map\n",
    "        # You can also directly use the point map head to get 3D points, but its performance is slightly worse than the depth map\n",
    "        depth_map, depth_conf = predictions[\"depth\"][0], predictions[\"depth_conf\"][0]\n",
    "        world_points = unproject_depth_map_to_point_map(depth_map, pred_extrinsic, pred_intrinsic)\n",
    "        world_points = torch.from_numpy(world_points).to(device)\n",
    "        world_points_conf = depth_conf.to(device)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Lists to store predictions\n",
    "    pred_tracks = []\n",
    "    pred_vis_scores = []\n",
    "    pred_conf_scores = []\n",
    "    pred_world_points = []\n",
    "    pred_world_points_conf = []\n",
    "\n",
    "    # Initialize feature extractors\n",
    "    extractors = initialize_feature_extractors(max_query_num, det_thres, extractor_method, device)\n",
    "\n",
    "    # Process each query frame\n",
    "    for query_index in query_frame_indexes:\n",
    "        query_image = images[query_index]\n",
    "        query_points_round = extract_keypoints(query_image, extractors, max_query_num)\n",
    "\n",
    "        # Reorder images to put query image first\n",
    "        reorder_index = calculate_index_mappings(query_index, frame_num, device=device)\n",
    "        reorder_images = switch_tensor_order([images], reorder_index, dim=0)[0]\n",
    "\n",
    "        # Track points across frames\n",
    "        reorder_tracks, reorder_vis_score, reorder_conf_score = predict_track(\n",
    "            model, reorder_images, query_points_round, dtype=dtype, use_tf32_for_track=True, iters=4,\n",
    "        )\n",
    "\n",
    "        # Restore original order\n",
    "        pred_track, pred_vis, pred_score = switch_tensor_order(\n",
    "            [reorder_tracks, reorder_vis_score, reorder_conf_score], reorder_index, dim=0\n",
    "        )\n",
    "\n",
    "        pred_tracks.append(pred_track)\n",
    "        pred_vis_scores.append(pred_vis)\n",
    "        pred_conf_scores.append(pred_score)\n",
    "\n",
    "        # Get corresponding 3D points\n",
    "        query_points_round_long = query_points_round.squeeze(0).long()\n",
    "        query_world_points = world_points[query_index][\n",
    "            query_points_round_long[:, 1], query_points_round_long[:, 0]\n",
    "        ]\n",
    "        query_world_points_conf = world_points_conf[query_index][\n",
    "            query_points_round_long[:, 1], query_points_round_long[:, 0]\n",
    "        ]\n",
    "\n",
    "        pred_world_points.append(query_world_points)\n",
    "        pred_world_points_conf.append(query_world_points_conf)\n",
    "\n",
    "    # Concatenate prediction lists\n",
    "    pred_tracks = torch.cat(pred_tracks, dim=1)\n",
    "    pred_vis_scores = torch.cat(pred_vis_scores, dim=1)\n",
    "    pred_conf_scores = torch.cat(pred_conf_scores, dim=1)\n",
    "    pred_world_points = torch.cat(pred_world_points, dim=0)\n",
    "    pred_world_points_conf = torch.cat(pred_world_points_conf, dim=0)\n",
    "\n",
    "    # Filter points by confidence\n",
    "    filtered_flag = pred_world_points_conf > 1.5\n",
    "\n",
    "    if filtered_flag.sum() > max_query_num // 2:\n",
    "        # Only filter if we have enough high-confidence points\n",
    "        pred_world_points = pred_world_points[filtered_flag]\n",
    "        pred_world_points_conf = pred_world_points_conf[filtered_flag]\n",
    "        pred_tracks = pred_tracks[:, filtered_flag]\n",
    "        pred_vis_scores = pred_vis_scores[:, filtered_flag]\n",
    "        pred_conf_scores = pred_conf_scores[:, filtered_flag]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Bundle adjustment parameters\n",
    "    S, _, H, W = images.shape\n",
    "    image_size = torch.tensor([W, H], dtype=pred_tracks.dtype, device=device)\n",
    "    masks = torch.logical_and(pred_vis_scores > 0.05, pred_conf_scores > 0.2)\n",
    "\n",
    "    # Convert to pycolmap format and run bundle adjustment\n",
    "    reconstruction, valid_track_mask = batch_matrix_to_pycolmap(\n",
    "        pred_world_points,\n",
    "        pred_extrinsic,\n",
    "        pred_intrinsic,\n",
    "        pred_tracks,\n",
    "        image_size,\n",
    "        masks=masks,\n",
    "        max_reproj_error=max_reproj_error,\n",
    "        shared_camera=shared_camera,\n",
    "        camera_type=camera_type,\n",
    "    )\n",
    "\n",
    "    if reconstruction is None:\n",
    "        return pred_extrinsic\n",
    "\n",
    "    ba_options = pycolmap.BundleAdjustmentOptions()\n",
    "    pycolmap.bundle_adjustment(reconstruction, ba_options)\n",
    "    _, pred_extrinsic, _, _ = pycolmap_to_batch_matrix(\n",
    "        reconstruction, device=device, camera_type=camera_type\n",
    "    )\n",
    "\n",
    "    return pred_extrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3354a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_model(device, model_path):\n",
    "    \"\"\"\n",
    "    Load the VGGT model.\n",
    "\n",
    "    Args:\n",
    "        device: Device to load the model on\n",
    "        model_path: Path to the model checkpoint\n",
    "\n",
    "    Returns:\n",
    "        Loaded VGGT model\n",
    "    \"\"\"\n",
    "    print(\"Initializing and loading VGGT model...\")\n",
    "    model = VGGT()\n",
    "    # _URL = \"https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt\"\n",
    "    # model.load_state_dict(torch.hub.load_state_dict_from_url(_URL))\n",
    "    print(f\"USING {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def set_random_seeds(seed):\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed value\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def convert_pt3d_RT_to_opencv(Rot, Trans):\n",
    "    \"\"\"\n",
    "    Convert Point3D extrinsic matrices to OpenCV convention.\n",
    "\n",
    "    Args:\n",
    "        Rot: 3D rotation matrix in Point3D format\n",
    "        Trans: 3D translation vector in Point3D format\n",
    "\n",
    "    Returns:\n",
    "        extri_opencv: 3x4 extrinsic matrix in OpenCV format\n",
    "    \"\"\"\n",
    "    rot_pt3d = np.array(Rot)\n",
    "    trans_pt3d = np.array(Trans)\n",
    "\n",
    "    trans_pt3d[:2] *= -1\n",
    "    rot_pt3d[:, :2] *= -1\n",
    "    rot_pt3d = rot_pt3d.transpose(1, 0)\n",
    "    extri_opencv = np.hstack((rot_pt3d, trans_pt3d[:, None]))\n",
    "    return extri_opencv\n",
    "\n",
    "\n",
    "def build_pair_index(N, B=1):\n",
    "    \"\"\"\n",
    "    Build indices for all possible pairs of frames.\n",
    "\n",
    "    Args:\n",
    "        N: Number of frames\n",
    "        B: Batch size\n",
    "\n",
    "    Returns:\n",
    "        i1, i2: Indices for all possible pairs\n",
    "    \"\"\"\n",
    "    i1_, i2_ = torch.combinations(torch.arange(N), 2, with_replacement=False).unbind(-1)\n",
    "    i1, i2 = [(i[None] + torch.arange(B)[:, None] * N).reshape(-1) for i in [i1_, i2_]]\n",
    "    return i1, i2\n",
    "\n",
    "def process_sequence(model, image_names, use_ba, device, dtype):\n",
    "    images = load_and_preprocess_images(image_names).to(device)\n",
    "    if use_ba:\n",
    "        try:\n",
    "            pred_extrinsic = run_vggt_with_ba(model, images, image_names=image_names, dtype=dtype)\n",
    "        except Exception as e:\n",
    "            print(f\"BA failed with error: {e}. Falling back to standard VGGT inference.\")\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast(dtype=dtype):\n",
    "                    predictions = model(images)\n",
    "            with torch.cuda.amp.autocast(dtype=dtype):\n",
    "                extrinsic, intrinsic = pose_encoding_to_extri_intri(predictions[\"pose_enc\"], images.shape[-2:])\n",
    "                pred_extrinsic = extrinsic[0]\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(dtype=dtype):\n",
    "                predictions = model(images)\n",
    "        with torch.cuda.amp.autocast(dtype=dtype):\n",
    "            extrinsic, intrinsic = pose_encoding_to_extri_intri(predictions[\"pose_enc\"], images.shape[-2:])\n",
    "            pred_extrinsic = extrinsic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c556d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\"\n",
    "dtype = torch.float16\n",
    "model = load_model(device, model_path=\"ckpts/vggt.pth\")\n",
    "set_random_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53451fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\"data/image-matching-challenge-2025/test/ETs/et_et000.png\", \n",
    "                \"data/image-matching-challenge-2025/test/ETs/et_et001.png\", \n",
    "                \"data/image-matching-challenge-2025/test/ETs/et_et002.png\", \n",
    "                \"data/image-matching-challenge-2025/test/ETs/et_et003.png\", \n",
    "                \"data/image-matching-challenge-2025/test/ETs/et_et004.png\",\n",
    "                \"data/image-matching-challenge-2025/test/ETs/et_et005.png\",\n",
    "                \"data/image-matching-challenge-2025/test/ETs/et_et006.png\"]\n",
    "process_sequence(model, image_names, use_ba=True, device=device, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
