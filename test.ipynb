{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mast3r.model import AsymmetricMASt3R\n",
    "from mast3r.fast_nn import fast_reciprocal_NNs\n",
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import copy\n",
    "from scipy.spatial.transform import Rotation\n",
    "import tempfile\n",
    "import shutil\n",
    "import torch\n",
    "import glob\n",
    "from mast3r.cloud_opt.sparse_ga import sparse_global_alignment\n",
    "from mast3r.cloud_opt.tsdf_optimizer import TSDFPostProcess\n",
    "from mast3r.image_pairs import make_pairs\n",
    "from mast3r.retrieval.processor import Retriever\n",
    "from mast3r.utils.misc import mkdir_for\n",
    "from cust3r.utils.image import load_images\n",
    "from dust3r.dust3r.utils.device import to_numpy\n",
    "from dust3r.dust3r.viz import add_scene_cam, CAM_COLORS, OPENGL, pts3d_to_trimesh, cat_meshes\n",
    "from dust3r.dust3r.demo import get_args_parser as dust3r_get_args_parser\n",
    "import matplotlib.pyplot as pl\n",
    "import imageio.v2 as iio\n",
    "from cust3r.utils.camera import pose_encoding_to_camera\n",
    "from cust3r.post_process import estimate_focal_knowing_depth\n",
    "from cust3r.utils.geometry import geotrf\n",
    "from cust3r.model import ARCroco3DStereo\n",
    "from cust3r.inference import inference as inference_cust3r\n",
    "import time\n",
    "from boq.boq_infer import get_trained_boq, boq_sort_topk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_scene_output_to_glb(imgs, pts3d, mask, focals, cams2world, cam_size=0.05,\n",
    "                                 cam_color=None, as_pointcloud=False,\n",
    "                                 transparent_cams=False, silent=False):\n",
    "    assert len(pts3d) == len(mask) <= len(imgs) <= len(cams2world) == len(focals)\n",
    "    pts3d = to_numpy(pts3d)\n",
    "    imgs = to_numpy(imgs)\n",
    "    focals = to_numpy(focals)\n",
    "    cams2world = to_numpy(cams2world)\n",
    "    scene = trimesh.Scene()\n",
    "    # full pointcloud\n",
    "    if as_pointcloud:\n",
    "        pts = np.concatenate([p[m.ravel()] for p, m in zip(pts3d, mask)]).reshape(-1, 3)\n",
    "        col = np.concatenate([p[m] for p, m in zip(imgs, mask)]).reshape(-1, 3)\n",
    "        valid_msk = np.isfinite(pts.sum(axis=1))\n",
    "        pct = trimesh.PointCloud(pts[valid_msk], colors=col[valid_msk])\n",
    "        scene.add_geometry(pct)\n",
    "    else:\n",
    "        meshes = []\n",
    "        for i in range(len(imgs)):\n",
    "            pts3d_i = pts3d[i].reshape(imgs[i].shape)\n",
    "            msk_i = mask[i] & np.isfinite(pts3d_i.sum(axis=-1))\n",
    "            meshes.append(pts3d_to_trimesh(imgs[i], pts3d_i, msk_i))\n",
    "        mesh = trimesh.Trimesh(**cat_meshes(meshes))\n",
    "        scene.add_geometry(mesh)\n",
    "    # add each camera\n",
    "    for i, pose_c2w in enumerate(cams2world):\n",
    "        if isinstance(cam_color, list):\n",
    "            camera_edge_color = cam_color[i]\n",
    "        else:\n",
    "            camera_edge_color = cam_color or CAM_COLORS[i % len(CAM_COLORS)]\n",
    "        add_scene_cam(scene, pose_c2w, camera_edge_color,\n",
    "                      None if transparent_cams else imgs[i], focals[i],\n",
    "                      imsize=imgs[i].shape[1::-1], screen_width=cam_size)\n",
    "    rot = np.eye(4)\n",
    "    rot[:3, :3] = Rotation.from_euler('y', np.deg2rad(180)).as_matrix()\n",
    "    scene.apply_transform(np.linalg.inv(cams2world[0] @ OPENGL @ rot))\n",
    "    return scene\n",
    "\n",
    "def get_3D_model_from_scene(silent, scene, min_conf_thr=2, as_pointcloud=False, mask_sky=False,\n",
    "                            clean_depth=False, transparent_cams=False, cam_size=0.05, TSDF_thresh=0):\n",
    "    \"\"\"\n",
    "    extract 3D_model (glb file) from a reconstructed scene\n",
    "    \"\"\"\n",
    "    # get optimized values from scene\n",
    "    scene = scene\n",
    "    rgbimg = scene.imgs\n",
    "    focals = scene.get_focals().cpu()\n",
    "    cams2world = scene.get_im_poses().cpu()\n",
    "    # 3D pointcloud from depthmap, poses and intrinsics\n",
    "    if TSDF_thresh > 0:\n",
    "        tsdf = TSDFPostProcess(scene, TSDF_thresh=TSDF_thresh)\n",
    "        pts3d, _, confs = to_numpy(tsdf.get_dense_pts3d(clean_depth=clean_depth))\n",
    "    else:\n",
    "        pts3d, _, confs = to_numpy(scene.get_dense_pts3d(clean_depth=clean_depth))\n",
    "    msk = to_numpy([c > min_conf_thr for c in confs])\n",
    "    return _convert_scene_output_to_glb(rgbimg, pts3d, msk, focals, cams2world, as_pointcloud=as_pointcloud,\n",
    "                                        transparent_cams=transparent_cams, cam_size=cam_size, silent=silent)\n",
    "    \n",
    "\n",
    "def get_reconstructed_scene(model, device, filelist,\n",
    "                            cache_path,\n",
    "                            retrieval_model = None,\n",
    "                            silent = False,\n",
    "                            optim_level = \"refine+depth\",\n",
    "                            lr1 = 0.07, niter1 = 200, lr2 = 0.01, niter2 = 200,\n",
    "                            min_conf_thr = 1.5,\n",
    "                            matching_conf_thr = 0.0,\n",
    "                            as_pointcloud = True, mask_sky = False, clean_depth =True, transparent_cams = False, cam_size = 0.2,\n",
    "                            scenegraph_type = \"complete\", winsize=1, win_cyclic=False, refid=0,\n",
    "                            TSDF_thresh=0.0, shared_intrinsics= False,\n",
    "                            **kw):\n",
    "    \"\"\"\n",
    "    from a list of images, run mast3r inference, sparse global aligner.\n",
    "    then run get_3D_model_from_scene\n",
    "    \"\"\"\n",
    "    imgs, imgs_id_dict = load_images(filelist, size=512, verbose=not silent)\n",
    "    if len(imgs) == 1:\n",
    "        imgs = [imgs[0], copy.deepcopy(imgs[0])]\n",
    "        imgs[1]['idx'] = 1\n",
    "        filelist = [filelist[0], filelist[0] + '_2']\n",
    "    scene_graph_params = [scenegraph_type]\n",
    "    if scenegraph_type in [\"swin\", \"logwin\"]:\n",
    "        scene_graph_params.append(str(winsize))\n",
    "    elif scenegraph_type == \"oneref\":\n",
    "        scene_graph_params.append(str(refid))\n",
    "    elif scenegraph_type == \"retrieval\":\n",
    "        scene_graph_params.append(str(winsize))  # Na\n",
    "        scene_graph_params.append(str(refid))  # k\n",
    "    if scenegraph_type in [\"swin\", \"logwin\"] and not win_cyclic:\n",
    "        scene_graph_params.append('noncyclic')\n",
    "    scene_graph = '-'.join(scene_graph_params)\n",
    "    sim_matrix = None\n",
    "    if 'retrieval' in scenegraph_type:\n",
    "        assert retrieval_model is not None\n",
    "        retriever = Retriever(retrieval_model, backbone=model, device=device)\n",
    "        with torch.no_grad():\n",
    "            sim_matrix = retriever(filelist)\n",
    "        # Cleanup\n",
    "        del retriever\n",
    "        torch.cuda.empty_cache()\n",
    "    boq_topks = None\n",
    "    if 'boq' in scenegraph_type:\n",
    "        with open(os.path.join(cache_path, \"boq_topk.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            boq_topks = json.load(f)\n",
    "    pairs = make_pairs(imgs, scene_graph=scene_graph, prefilter=None, \n",
    "                       symmetrize=False, sim_mat=sim_matrix, boq_topk_dict=boq_topks, \n",
    "                       imgs_id_dict=imgs_id_dict)\n",
    "    if optim_level == 'coarse':\n",
    "        niter2 = 0\n",
    "    # Sparse GA (forward mast3r -> matching -> 3D optim -> 2D refinement -> triangulation)\n",
    "    scenes, outlier_imgs = sparse_global_alignment(filelist, imgs, imgs_id_dict, pairs, cache_path,\n",
    "                                    model, lr1=lr1, niter1=niter1, lr2=lr2, niter2=niter2, device=device,\n",
    "                                    opt_depth='depth' in optim_level, shared_intrinsics=shared_intrinsics,\n",
    "                                    matching_conf_thr=matching_conf_thr, **kw)\n",
    "    trimesh_scenes = []\n",
    "    for i, scene in enumerate(scenes):\n",
    "        trimesh_scene = get_3D_model_from_scene(silent, scene, min_conf_thr, as_pointcloud, mask_sky,\n",
    "                                      clean_depth, transparent_cams, cam_size, TSDF_thresh)\n",
    "        trimesh_scenes.append(trimesh_scene)\n",
    "    return trimesh_scenes, outlier_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "model = AsymmetricMASt3R.from_pretrained(\"ckpts/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\").to(device)\n",
    "image_list = []\n",
    "for filename in glob.glob('data/image-matching-challenge-2025/train/imc2023_haiper/*.png'): #assuming gif\n",
    "    image_list.append(filename)\n",
    "\n",
    "boq_model = get_trained_boq(backbone_name=\"dinov2\", output_dim=12288, ckpt='ckpts/dinov2_12288.pth').to(device)\n",
    "boq_model.eval()\n",
    "boq_topks = boq_sort_topk(image_list, boq_model, device, vis=False, topk=32)\n",
    "\n",
    "os.makedirs(\"outputs/imc2023_haiper\", exist_ok=True)\n",
    "with open(os.path.join(\"outputs/imc2023_haiper\", \"boq_topk.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(boq_topks, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "trimesh_scenes, outlier_imgs = get_reconstructed_scene(model, device, image_list, \"outputs/imc2023_haiper\", scenegraph_type = \"boq\")\n",
    "del model, boq_model\n",
    "torch.cuda.empty_cache()\n",
    "trimesh_scenes[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimesh_scenes[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_image_same_size(images):\n",
    "    if len(images) == 0:\n",
    "        return\n",
    "    # Get the size of the first image\n",
    "    first_image_size = images[0][\"img\"].shape[-2:]\n",
    "    first_image_true_shape = images[0][\"true_shape\"]\n",
    "    for i in range(1, len(images)):\n",
    "        if images[i][\"img\"].shape[-2:] == first_image_size:\n",
    "            continue\n",
    "        # Resize the image to match the first image size\n",
    "        images[i][\"img\"] = torch.nn.functional.interpolate(\n",
    "            images[i][\"img\"],\n",
    "            size=first_image_size,\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        images[i][\"true_shape\"] = first_image_true_shape.copy()\n",
    "    \n",
    "    \n",
    "\n",
    "def prepare_input(\n",
    "    img_paths, img_mask, size, raymaps=None, raymap_mask=None, revisit=1, update=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare input views for inference from a list of image paths.\n",
    "\n",
    "    Args:\n",
    "        img_paths (list): List of image file paths.\n",
    "        img_mask (list of bool): Flags indicating valid images.\n",
    "        size (int): Target image size.\n",
    "        raymaps (list, optional): List of ray maps.\n",
    "        raymap_mask (list, optional): Flags indicating valid ray maps.\n",
    "        revisit (int): How many times to revisit each view.\n",
    "        update (bool): Whether to update the state on revisits.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of view dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    images = load_images(img_paths, size=size)\n",
    "    make_all_image_same_size(images)\n",
    "    views = []\n",
    "\n",
    "    if raymaps is None and raymap_mask is None:\n",
    "        # Only images are provided.\n",
    "        for i in range(len(images)):\n",
    "            view = {\n",
    "                \"img\": images[i][\"img\"],\n",
    "                \"ray_map\": torch.full(\n",
    "                    (\n",
    "                        images[i][\"img\"].shape[0],\n",
    "                        6,\n",
    "                        images[i][\"img\"].shape[-2],\n",
    "                        images[i][\"img\"].shape[-1],\n",
    "                    ),\n",
    "                    torch.nan,\n",
    "                ),\n",
    "                \"true_shape\": torch.from_numpy(images[i][\"true_shape\"]),\n",
    "                \"idx\": i,\n",
    "                \"instance\": str(i),\n",
    "                \"camera_pose\": torch.from_numpy(np.eye(4, dtype=np.float32)).unsqueeze(\n",
    "                    0\n",
    "                ),\n",
    "                \"img_mask\": torch.tensor(True).unsqueeze(0),\n",
    "                \"ray_mask\": torch.tensor(False).unsqueeze(0),\n",
    "                \"update\": torch.tensor(True).unsqueeze(0),\n",
    "                \"reset\": torch.tensor(False).unsqueeze(0),\n",
    "            }\n",
    "            views.append(view)\n",
    "    else:\n",
    "        # Combine images and raymaps.\n",
    "        num_views = len(images) + len(raymaps)\n",
    "        assert len(img_mask) == len(raymap_mask) == num_views\n",
    "        assert sum(img_mask) == len(images) and sum(raymap_mask) == len(raymaps)\n",
    "\n",
    "        j = 0\n",
    "        k = 0\n",
    "        for i in range(num_views):\n",
    "            view = {\n",
    "                \"img\": (\n",
    "                    images[j][\"img\"]\n",
    "                    if img_mask[i]\n",
    "                    else torch.full_like(images[0][\"img\"], torch.nan)\n",
    "                ),\n",
    "                \"ray_map\": (\n",
    "                    raymaps[k]\n",
    "                    if raymap_mask[i]\n",
    "                    else torch.full_like(raymaps[0], torch.nan)\n",
    "                ),\n",
    "                \"true_shape\": (\n",
    "                    torch.from_numpy(images[j][\"true_shape\"])\n",
    "                    if img_mask[i]\n",
    "                    else torch.from_numpy(np.int32([raymaps[k].shape[1:-1][::-1]]))\n",
    "                ),\n",
    "                \"idx\": i,\n",
    "                \"instance\": str(i),\n",
    "                \"camera_pose\": torch.from_numpy(np.eye(4, dtype=np.float32)).unsqueeze(\n",
    "                    0\n",
    "                ),\n",
    "                \"img_mask\": torch.tensor(img_mask[i]).unsqueeze(0),\n",
    "                \"ray_mask\": torch.tensor(raymap_mask[i]).unsqueeze(0),\n",
    "                \"update\": torch.tensor(img_mask[i]).unsqueeze(0),\n",
    "                \"reset\": torch.tensor(False).unsqueeze(0),\n",
    "            }\n",
    "            if img_mask[i]:\n",
    "                j += 1\n",
    "            if raymap_mask[i]:\n",
    "                k += 1\n",
    "            views.append(view)\n",
    "        assert j == len(images) and k == len(raymaps)\n",
    "\n",
    "    if revisit > 1:\n",
    "        new_views = []\n",
    "        for r in range(revisit):\n",
    "            for i, view in enumerate(views):\n",
    "                new_view = copy.deepcopy(view)\n",
    "                new_view[\"idx\"] = r * len(views) + i\n",
    "                new_view[\"instance\"] = str(r * len(views) + i)\n",
    "                if r > 0 and not update:\n",
    "                    new_view[\"update\"] = torch.tensor(False).unsqueeze(0)\n",
    "                new_views.append(new_view)\n",
    "        return new_views\n",
    "\n",
    "    return views\n",
    "\n",
    "def prepare_output(outputs, outdir, revisit=1, use_pose=True):\n",
    "    \"\"\"\n",
    "    Process inference outputs to generate point clouds and camera parameters for visualization.\n",
    "    Args:\n",
    "        outputs (dict): Inference outputs.\n",
    "        revisit (int): Number of revisits per view.\n",
    "        use_pose (bool): Whether to transform points using camera pose.\n",
    "    Returns:\n",
    "        tuple: (points, colors, confidence, camera parameters dictionary)\n",
    "    \"\"\"\n",
    "    # Only keep the outputs corresponding to one full pass.\n",
    "    valid_length = len(outputs[\"pred\"]) // revisit\n",
    "    outputs[\"pred\"] = outputs[\"pred\"][-valid_length:]\n",
    "    outputs[\"views\"] = outputs[\"views\"][-valid_length:]\n",
    "    pts3ds_self_ls = [output[\"pts3d_in_self_view\"].cpu() for output in outputs[\"pred\"]]\n",
    "    pts3ds_other = [output[\"pts3d_in_other_view\"].cpu() for output in outputs[\"pred\"]]\n",
    "    conf_self = [output[\"conf_self\"].cpu() for output in outputs[\"pred\"]]\n",
    "    conf_other = [output[\"conf\"].cpu() for output in outputs[\"pred\"]]\n",
    "    pts3ds_self = torch.cat(pts3ds_self_ls, 0)\n",
    "    # Recover camera poses.\n",
    "    pr_poses = [\n",
    "        pose_encoding_to_camera(pred[\"camera_pose\"].clone()).cpu()\n",
    "        for pred in outputs[\"pred\"]\n",
    "    ]\n",
    "    R_c2w = torch.cat([pr_pose[:, :3, :3] for pr_pose in pr_poses], 0)\n",
    "    t_c2w = torch.cat([pr_pose[:, :3, 3] for pr_pose in pr_poses], 0)\n",
    "    if use_pose:\n",
    "        transformed_pts3ds_other = []\n",
    "        for pose, pself in zip(pr_poses, pts3ds_self):\n",
    "            transformed_pts3ds_other.append(geotrf(pose, pself.unsqueeze(0)))\n",
    "        pts3ds_other = transformed_pts3ds_other\n",
    "        conf_other = conf_self\n",
    "    # Estimate focal length based on depth.\n",
    "    B, H, W, _ = pts3ds_self.shape\n",
    "    pp = torch.tensor([W // 2, H // 2], device=pts3ds_self.device).float().repeat(B, 1)\n",
    "    focal = estimate_focal_knowing_depth(pts3ds_self, pp, focal_mode=\"weiszfeld\")\n",
    "    colors = [\n",
    "        0.5 * (output[\"img\"].permute(0, 2, 3, 1) + 1.0) for output in outputs[\"views\"]\n",
    "    ]\n",
    "    cam_dict = {\n",
    "        \"focal\": focal.cpu().numpy(),\n",
    "        \"pp\": pp.cpu().numpy(),\n",
    "        \"R\": R_c2w.cpu().numpy(),\n",
    "        \"t\": t_c2w.cpu().numpy(),\n",
    "        \"cams2world\": torch.cat(pr_poses).cpu().numpy(),\n",
    "    }\n",
    "    pts3ds_self_tosave = pts3ds_self  # B, H, W, 3\n",
    "    depths_tosave = pts3ds_self_tosave[..., 2]\n",
    "    pts3ds_other_tosave = torch.cat(pts3ds_other)  # B, H, W, 3\n",
    "    conf_self_tosave = torch.cat(conf_self)  # B, H, W\n",
    "    conf_other_tosave = torch.cat(conf_other)  # B, H, W\n",
    "    colors_tosave = torch.cat(\n",
    "        [\n",
    "            0.5 * (output[\"img\"].permute(0, 2, 3, 1).cpu() + 1.0)\n",
    "            for output in outputs[\"views\"]\n",
    "        ]\n",
    "    )  # [B, H, W, 3]\n",
    "    cam2world_tosave = torch.cat(pr_poses)  # B, 4, 4\n",
    "    intrinsics_tosave = (\n",
    "        torch.eye(3).unsqueeze(0).repeat(cam2world_tosave.shape[0], 1, 1)\n",
    "    )  # B, 3, 3\n",
    "    intrinsics_tosave[:, 0, 0] = focal.detach().cpu()\n",
    "    intrinsics_tosave[:, 1, 1] = focal.detach().cpu()\n",
    "    intrinsics_tosave[:, 0, 2] = pp[:, 0]\n",
    "    intrinsics_tosave[:, 1, 2] = pp[:, 1]\n",
    "    os.makedirs(os.path.join(outdir, \"depth\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(outdir, \"conf\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(outdir, \"color\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(outdir, \"camera\"), exist_ok=True)\n",
    "    for f_id in range(len(pts3ds_self)):\n",
    "        depth = depths_tosave[f_id].cpu().numpy()\n",
    "        conf = conf_self_tosave[f_id].cpu().numpy()\n",
    "        color = colors_tosave[f_id].cpu().numpy()\n",
    "        c2w = cam2world_tosave[f_id].cpu().numpy()\n",
    "        intrins = intrinsics_tosave[f_id].cpu().numpy()\n",
    "        np.save(os.path.join(outdir, \"depth\", f\"{f_id:06d}.npy\"), depth)\n",
    "        np.save(os.path.join(outdir, \"conf\", f\"{f_id:06d}.npy\"), conf)\n",
    "        iio.imwrite(\n",
    "            os.path.join(outdir, \"color\", f\"{f_id:06d}.png\"),\n",
    "            (color * 255).astype(np.uint8),\n",
    "        )\n",
    "        np.savez(\n",
    "            os.path.join(outdir, \"camera\", f\"{f_id:06d}.npz\"),\n",
    "            pose=c2w,\n",
    "            intrinsics=intrins,\n",
    "        )\n",
    "    return pts3ds_other, colors, conf_other, cam_dict\n",
    "\n",
    "def run_inference(model, image_list, output_dir, device, min_conf_thr = 2.0, size = 512,\n",
    "                  transparent_cams=False, cam_size=0.2, as_pointcloud = True, silent= True):\n",
    "    \"\"\"\n",
    "    Execute the full inference and visualization pipeline.\n",
    "    Args:\n",
    "        args: Parsed command-line arguments.\n",
    "    \"\"\"\n",
    "    # Set up the computation device.\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available. Switching to CPU.\")\n",
    "        device = \"cpu\"\n",
    "    # Prepare image file paths.\n",
    "    img_mask = [True] * len(image_list)\n",
    "    # Prepare input views.\n",
    "    print(\"Preparing input views...\")\n",
    "    views = prepare_input(img_paths=image_list, img_mask=img_mask, size=size, revisit=1, update=True)\n",
    "    # Run inference.\n",
    "    print(\"Running inference...\")\n",
    "    outputs, state_args = inference_cust3r(views, model, device)\n",
    "    # Process outputs for visualization.\n",
    "    print(\"Preparing output for visualization...\")\n",
    "    pts3ds_other, colors, conf, cam_dict = prepare_output(outputs, output_dir, 1, True)\n",
    "    # Convert tensors to numpy arrays for visualization.\n",
    "    pts3ds_to_vis = [p.reshape((-1, 3)).cpu().numpy() for p in pts3ds_other]\n",
    "    colors_to_vis = [c.squeeze(0).cpu().numpy() for c in colors]\n",
    "    focals = cam_dict[\"focal\"]\n",
    "    cams2world = cam_dict[\"cams2world\"]\n",
    "    msk = [(c > min_conf_thr).squeeze(0).cpu().numpy() for c in conf]\n",
    "    return _convert_scene_output_to_glb(colors_to_vis, pts3ds_to_vis, msk, focals, cams2world, as_pointcloud=as_pointcloud,\n",
    "                                        transparent_cams=transparent_cams, cam_size=cam_size, silent=silent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "print(f\"Loading model ...\")\n",
    "model = ARCroco3DStereo.from_pretrained(\"ckpts/cut3r_512_dpt_4_64.pth\").to(device)\n",
    "model.eval()\n",
    "image_list = []\n",
    "for filename in glob.glob('data/image-matching-challenge-2025/train/11/*.png'):\n",
    "    image_list.append(filename)\n",
    "trimesh_scene = run_inference(model, image_list, \"outputs/cust/1111\", device, 3.0)\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trimesh_scene.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
