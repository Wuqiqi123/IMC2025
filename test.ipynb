{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mast3r.model import AsymmetricMASt3R\n",
    "from mast3r.fast_nn import fast_reciprocal_NNs\n",
    "\n",
    "# import mast3r.utils.path_to_dust3r\n",
    "from dust3r.dust3r.inference import inference\n",
    "from dust3r.dust3r.utils.image import load_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading model from ckpts/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\n",
      "instantiating : AsymmetricMASt3R(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100',img_size=(512, 512), head_type='catmlp+dpt', output_mode='pts3d+desc24', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), patch_embed_cls='PatchEmbedDust3R', two_confs=True, desc_conf_mode=('exp', 0, inf), landscape_only=False)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CroCoNet.__init__() got an unexpected keyword argument 'enc_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpts/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# you can put the path to a local checkpoint in model_name if needed\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAsymmetricMASt3R\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m images \u001b[38;5;241m=\u001b[39m load_images([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcroco/assets/Chateau1.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcroco/assets/Chateau2.png\u001b[39m\u001b[38;5;124m'\u001b[39m], size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     10\u001b[0m output \u001b[38;5;241m=\u001b[39m inference([\u001b[38;5;28mtuple\u001b[39m(images)], model, device, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/workspace/IMC2025/mast3r/model.py:50\u001b[0m, in \u001b[0;36mAsymmetricMASt3R.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kw)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(pretrained_model_name_or_path):\n\u001b[0;32m---> 50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(AsymmetricMASt3R, \u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/workspace/IMC2025/mast3r/model.py:33\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_path, device, verbose)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstantiating : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m s \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "File \u001b[0;32m/workspace/IMC2025/mast3r/model.py:45\u001b[0m, in \u001b[0;36mAsymmetricMASt3R.__init__\u001b[0;34m(self, desc_mode, two_confs, desc_conf_mode, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtwo_confs \u001b[38;5;241m=\u001b[39m two_confs\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesc_conf_mode \u001b[38;5;241m=\u001b[39m desc_conf_mode\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/IMC2025/dust3r/dust3r/model.py:69\u001b[0m, in \u001b[0;36mAsymmetricCroCo3DStereo.__init__\u001b[0;34m(self, output_mode, head_type, depth_mode, conf_mode, freeze, landscape_only, patch_embed_cls, **croco_kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed_cls \u001b[38;5;241m=\u001b[39m patch_embed_cls\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcroco_args \u001b[38;5;241m=\u001b[39m fill_default_args(croco_kwargs, \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcroco_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# dust3r specific initialization\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_blocks2 \u001b[38;5;241m=\u001b[39m deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_blocks)\n",
      "\u001b[0;31mTypeError\u001b[0m: CroCoNet.__init__() got an unexpected keyword argument 'enc_depth'"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "schedule = 'cosine'\n",
    "lr = 0.01\n",
    "niter = 300\n",
    "\n",
    "model_name = \"ckpts/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\"\n",
    "# you can put the path to a local checkpoint in model_name if needed\n",
    "model = AsymmetricMASt3R.from_pretrained(model_name).to(device)\n",
    "images = load_images(['croco/assets/Chateau1.png', 'croco/assets/Chateau2.png'], size=512)\n",
    "output = inference([tuple(images)], model, device, batch_size=1, verbose=False)\n",
    "\n",
    "# at this stage, you have the raw dust3r predictions\n",
    "view1, pred1 = output['view1'], output['pred1']\n",
    "view2, pred2 = output['view2'], output['pred2']\n",
    "\n",
    "desc1, desc2 = pred1['desc'].squeeze(0).detach(), pred2['desc'].squeeze(0).detach()\n",
    "\n",
    "# find 2D-2D matches between the two images\n",
    "matches_im0, matches_im1 = fast_reciprocal_NNs(desc1, desc2, subsample_or_initxy1=8,\n",
    "                                                device=device, dist='dot', block_size=2**13)\n",
    "\n",
    "# ignore small border around the edge\n",
    "H0, W0 = view1['true_shape'][0]\n",
    "valid_matches_im0 = (matches_im0[:, 0] >= 3) & (matches_im0[:, 0] < int(W0) - 3) & (\n",
    "    matches_im0[:, 1] >= 3) & (matches_im0[:, 1] < int(H0) - 3)\n",
    "\n",
    "H1, W1 = view2['true_shape'][0]\n",
    "valid_matches_im1 = (matches_im1[:, 0] >= 3) & (matches_im1[:, 0] < int(W1) - 3) & (\n",
    "    matches_im1[:, 1] >= 3) & (matches_im1[:, 1] < int(H1) - 3)\n",
    "\n",
    "valid_matches = valid_matches_im0 & valid_matches_im1\n",
    "matches_im0, matches_im1 = matches_im0[valid_matches], matches_im1[valid_matches]\n",
    "\n",
    "# visualize a few matches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional\n",
    "from matplotlib import pyplot as pl\n",
    "\n",
    "n_viz = 20\n",
    "num_matches = matches_im0.shape[0]\n",
    "match_idx_to_viz = np.round(np.linspace(0, num_matches - 1, n_viz)).astype(int)\n",
    "viz_matches_im0, viz_matches_im1 = matches_im0[match_idx_to_viz], matches_im1[match_idx_to_viz]\n",
    "\n",
    "image_mean = torch.as_tensor([0.5, 0.5, 0.5], device='cpu').reshape(1, 3, 1, 1)\n",
    "image_std = torch.as_tensor([0.5, 0.5, 0.5], device='cpu').reshape(1, 3, 1, 1)\n",
    "\n",
    "viz_imgs = []\n",
    "for i, view in enumerate([view1, view2]):\n",
    "    rgb_tensor = view['img'] * image_std + image_mean\n",
    "    viz_imgs.append(rgb_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "H0, W0, H1, W1 = *viz_imgs[0].shape[:2], *viz_imgs[1].shape[:2]\n",
    "img0 = np.pad(viz_imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)\n",
    "img1 = np.pad(viz_imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)\n",
    "img = np.concatenate((img0, img1), axis=1)\n",
    "pl.figure()\n",
    "pl.imshow(img)\n",
    "cmap = pl.get_cmap('jet')\n",
    "for i in range(n_viz):\n",
    "    (x0, y0), (x1, y1) = viz_matches_im0[i].T, viz_matches_im1[i].T\n",
    "    pl.plot([x0, x1 + W0], [y0, y1], '-+', color=cmap(i / (n_viz - 1)), scalex=False, scaley=False)\n",
    "pl.show(block=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading model from ckpts/cut3r_224_linear_4.pth\n",
      "instantiating : ARCroco3DStereo(ARCroco3DStereoConfig(freeze='encoder_and_head', state_size=768, state_pe='2d', pos_embed='RoPE100', rgb_head=True, pose_head=True,  img_size=(224, 224), head_type='linear', output_mode='pts3d+pose', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), pose_mode=('exp', -inf, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, landscape_only=False))\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ARCroco3DStereo(\n",
       "  (patch_embed): PatchEmbedDust3R(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (patch_embed_ray_map): PatchEmbedDust3R(\n",
       "    (proj): Conv2d(6, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (mask_generator): RandomMask()\n",
       "  (rope): cuRoPE2D()\n",
       "  (enc_blocks): ModuleList(\n",
       "    (0-23): 24 x Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (rope): cuRoPE2D()\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (enc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (decoder_embed): Linear(in_features=1024, out_features=768, bias=True)\n",
       "  (dec_blocks): ModuleList(\n",
       "    (0-11): 12 x DecoderBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (rope): cuRoPE2D()\n",
       "      )\n",
       "      (cross_attn): CrossAttention(\n",
       "        (projq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (projk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (projv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (rope): cuRoPE2D()\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_y): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dec_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (enc_blocks_ray_map): ModuleList(\n",
       "    (0-1): 2 x Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (rope): cuRoPE2D()\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (enc_norm_ray_map): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (pose_retriever): LocalMemory(\n",
       "    (proj_q): Linear(in_features=1024, out_features=768, bias=True)\n",
       "    (write_blocks): ModuleList(\n",
       "      (0-1): 2 x DecoderBlock(\n",
       "        (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (cross_attn): CrossAttention(\n",
       "          (projq): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (projk): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (projv): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm_y): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (read_blocks): ModuleList(\n",
       "      (0-1): 2 x DecoderBlock(\n",
       "        (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (cross_attn): CrossAttention(\n",
       "          (projq): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (projk): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (projv): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm_y): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (register_tokens): Embedding(768, 1024)\n",
       "  (decoder_embed_state): Linear(in_features=1024, out_features=768, bias=True)\n",
       "  (dec_blocks_state): ModuleList(\n",
       "    (0-11): 12 x DecoderBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (rope): cuRoPE2D()\n",
       "      )\n",
       "      (cross_attn): CrossAttention(\n",
       "        (projq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (projk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (projv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (rope): cuRoPE2D()\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_y): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dec_norm_state): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (downstream_head): LinearPts3dPose(\n",
       "    (proj): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (fc2): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (rgb_proj): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (pose_head): PoseDecoder(\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=7, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_transform): ModuleList(\n",
       "      (0-1): 2 x ConditionModulationBlock(\n",
       "        (norm1): ModLN(\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): cuRoPE2D()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): ModLN(\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cross_proj): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (fc2): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cust3r.inference import inference, inference_recurrent\n",
    "from cust3r.model import ARCroco3DStereo\n",
    "cust3r_model_name = \"ckpts/cut3r_224_linear_4.pth\"\n",
    "device = 'cuda'\n",
    "model = ARCroco3DStereo.from_pretrained(cust3r_model_name).to(device)\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
